# PySpark-Optimization
Project: Spark Optimization using PySpark

In this project, I explored advanced PySpark optimization techniques to improve performance and scalability in distributed data processing. The hands-on implementation covered key Spark components, resource management strategies, and optimization mechanisms.

Key learnings and topics covered:

Setting up and configuring Spark clusters and sessions in Databricks

Partitioning and scanning optimization for efficient data reads

Join optimizations including Broadcast Joins and Sort-Merge Joins

Using Spark SQL Hints to guide the optimizer

Caching and persistence strategies for iterative workloads

Understanding Dynamic Resource Allocation for cluster efficiency

Adaptive Query Execution (AQE) for runtime plan optimization

Dynamic Partition Pruning to reduce shuffle and read overhead

Leveraging Broadcast Variables for small lookup datasets

Salting techniques to handle data skew

Delta Lake optimization for ACID transactions and data reliability

Tools & Technologies:
PySpark, Databricks, Delta Lake, Spark SQL, Python
